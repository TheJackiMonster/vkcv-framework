#version 440
#extension GL_GOOGLE_include_directive : enable
#include "motionBlurConfig.inc"

layout(set=0, binding=0)                    uniform texture2D   inColor;
layout(set=0, binding=1)                    uniform texture2D   inDepth;
layout(set=0, binding=2)                    uniform texture2D   inMotionFullRes;
layout(set=0, binding=3)                    uniform texture2D   inMotionNeighbourhoodMax;  
layout(set=0, binding=4)                    uniform sampler     nearestSampler;
layout(set=0, binding=5, r11f_g11f_b10f)    uniform image2D     outImage;

layout(local_size_x = 8, local_size_y = 8, local_size_z = 1) in;

layout( push_constant ) uniform constants{
    float motionScaleFactor;  // computed from delta time and shutter speed
    float minVelocity;
    // camera planes are needed to linearize depth
    float cameraNearPlane;
    float cameraFarPlane;
};

float linearizeDepth(float depth, float near, float far){
    return near * far / (far + depth * (near - far));
}

struct SampleData{
    vec3    color;
    float   depthLinear;
    vec2    uv;
    vec2    motion;
    float   velocity;
};

// estimates if a points lies within the influence of another point
// uv1 and uv2 can be interchanged, the velocity belongs to the point whose influence is estimated
float cone(vec2 uv1, vec2 uv2, float velocity){
    return clamp(1 - distance(uv1, uv2) / velocity, 0, 1);
}

// similar to cone, but with a different shape
// see paper for usage details
float cylinder(vec2 uv1, vec2 uv2, float velocity){
    return 1 - smoothstep(0.95 * velocity, 1.05 * velocity, distance(uv1, uv2));
}

// checks if depth1 is closer than depth2, result within range [0, 1]
float softDepthCompare(float depth1, float depth2){
    float softDepthExtent = 0.0001;
    return clamp(1 - (depth1 - depth2) / softDepthExtent, 0, 1);
}

// reconstruction filter and helper functions from "A Reconstruction Filter for Plausible Motion Blur", McGuire
float computeSampleWeigth(SampleData mainPixel, SampleData samplePixel){
    
    float foreground = softDepthCompare(samplePixel.depthLinear,    mainPixel.depthLinear);
    float background = softDepthCompare(  mainPixel.depthLinear,  samplePixel.depthLinear);
    
    float weight = 0;
    
    // blurry sample in front of main pixel
    weight += foreground * cone(mainPixel.uv, samplePixel.uv, samplePixel.velocity);
    
    // any sample behind blurry main pixel: estimate background by using sample
    weight += background * cone(mainPixel.uv, samplePixel.uv, mainPixel.velocity);
    
    // both main pixel and sample are blurry and overlap
    weight += 2 * cylinder(mainPixel.uv, samplePixel.uv, mainPixel.velocity) * cylinder(mainPixel.uv, samplePixel.uv, samplePixel.velocity);

    return weight;
}

// see "A Reconstruction Filter for Plausible Motion Blur", section 2.2
vec2 processMotionVector(vec2 motion){
    // every frame a pixel should blur over the distance it moves
    // as we blur in two directions (where it was and where it will be) we must half the motion 
    vec2 motionHalf     = motion * 0.5;
    vec2 motionScaled   = motionHalf * motionScaleFactor; // scale factor contains shutter speed and delta time
    
    // pixels are anisotropic so the smaller dimension is used, so the clamping is conservative
    float pixelSize     = 1.f / max(imageSize(outImage).x, imageSize(outImage).y);
    float velocity      = length(motionScaled);
    float epsilon       = 0.0001;
    // this clamps the motion to not exceed the radius given by the motion tile size
    return motionScaled * max(0.5 * pixelSize, min(velocity, motionTileSize * pixelSize)) / (velocity + epsilon);
}

SampleData loadSampleData(vec2 uv){
    
    SampleData data;
    data.color          = texture(sampler2D(inColor, nearestSampler), uv).rgb;
    data.uv             = (ivec2(uv * imageSize(outImage)) + 0.5) / imageSize(outImage);    // quantize to integer coordinates, then move to pixel center and compute final uv
    data.motion         = processMotionVector(texture(sampler2D(inMotionFullRes, nearestSampler), uv).rg);
    data.velocity       = length(data.motion);
    data.depthLinear    = texture(sampler2D(inDepth, nearestSampler), uv).r;
    data.depthLinear    = linearizeDepth(data.depthLinear, cameraNearPlane, cameraFarPlane);
    
    return data;
}

// simple binary dither pattern
// could be optimized to avoid modulo and branch
float dither(ivec2 coord){
    
    int ditherSize = 4;
    
    bool x = coord.x % ditherSize < (ditherSize / 2);
    bool y = coord.y % ditherSize < (ditherSize / 2);
    
    return x ^^ y ? 1 : 0;
}

void main(){

    if(any(greaterThanEqual(gl_GlobalInvocationID.xy, imageSize(outImage))))
        return;
   
    ivec2   textureRes  = textureSize(sampler2D(inColor, nearestSampler), 0);
    ivec2   coord       = ivec2(gl_GlobalInvocationID.xy);
    vec2    uv          = vec2(coord + 0.5) / textureRes;   // + 0.5 to shift uv into pixel center
    
    vec2 motionNeighbourhoodMax = processMotionVector(texture(sampler2D(inMotionNeighbourhoodMax, nearestSampler), uv).rg);
    
    SampleData mainPixel = loadSampleData(uv);
    
    // early out on little movement
    if(length(motionNeighbourhoodMax) <= minVelocity){
        imageStore(outImage, coord, vec4(mainPixel.color, 0.f));
        return;
    }
    
    // the main pixel always contributes to the motion blur
    // however if it is spread across multiple pixels, it should distribute it's color evenly among all of them (assuming a linear motion)
    // because of this the pixel motion is translated into pixels
    // for example if a pixel covers a five pixel distance, then it's weight is 1 / 5
    float   mainPixelCoverageLength = max(length(mainPixel.motion * imageSize(outImage)), 1);   // max 1 because a pixel can't cover less than it's size
    float   mainPixelWeight         = 1.f / mainPixelCoverageLength;
    
    vec3    color           = mainPixel.color * mainPixelWeight;
    float   weightSum       = mainPixelWeight;
    
    const int sampleCount = 15; 
    
    // clamping start and end points avoids artifacts at image borders
    // the sampler clamps the sample uvs anyways, but without clamping here, many samples can be stuck at the border
    vec2 uvStart    = clamp(uv - motionNeighbourhoodMax, 0, 1);
    vec2 uvEnd      = clamp(uv + motionNeighbourhoodMax, 0, 1);
    
    // samples are placed evenly, but the entire filter is jittered
    // dither returns either 0 or 1
    // the sampleUV code expects an offset in range [-0.5, 0.5], so the dither is rescaled to a binary -0.25/0.25
    float random = dither(coord) * 0.5 - 0.25;
    
    for(int i = 0; i < sampleCount; i++){
        vec2 sampleUV = mix(uvStart, uvEnd, (i + random + 1) / float(sampleCount + 1));
        
        SampleData  samplePixel     = loadSampleData(sampleUV);
        float       weightSample    = computeSampleWeigth(mainPixel, samplePixel);

        weightSum   += weightSample;
        color       += samplePixel.color * weightSample;
    }
    
    color /= weightSum;

    imageStore(outImage, coord, vec4(color, 0.f));
}